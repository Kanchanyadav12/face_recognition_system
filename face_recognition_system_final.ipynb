{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c72c7489-728a-4eeb-8d83-d0ac80cb0300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\kanch\\facenet_env\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\kanch\\facenet_env\\lib\\site-packages (from opencv-python) (1.24.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\kanch\\facenet_env\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\kanch\\facenet_env\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\kanch\\facenet_env\\lib\\site-packages (from opencv-contrib-python) (1.24.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\kanch\\facenet_env\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n",
    "!pip install opencv-contrib-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f12998f-4c42-4ea4-9807-212794cf82b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: C:\\Users\\kanch\\OneDrive\\Desktop\\face recognition system\\images\\kanchan\\WhatsApp Image 2025-06-08 at 12.18.28_1ef9c98c.jpg\n",
      "Saved: processed_images\\kanchan_WhatsApp Image 2025-06-08 at 12.18.28_1ef9c98c.jpg\n",
      "Reading: C:\\Users\\kanch\\OneDrive\\Desktop\\face recognition system\\images\\kanchan\\WhatsApp Image 2025-06-08 at 12.18.29_31a2bba0.jpg\n",
      "Saved: processed_images\\kanchan_WhatsApp Image 2025-06-08 at 12.18.29_31a2bba0.jpg\n",
      "Reading: C:\\Users\\kanch\\OneDrive\\Desktop\\face recognition system\\images\\kanchan\\WhatsApp Image 2025-06-08 at 12.18.30_625eb08d.jpg\n",
      "Saved: processed_images\\kanchan_WhatsApp Image 2025-06-08 at 12.18.30_625eb08d.jpg\n",
      "Reading: C:\\Users\\kanch\\OneDrive\\Desktop\\face recognition system\\images\\kanchan\\WhatsApp Image 2025-06-08 at 12.18.31_da1b067b.jpg\n",
      "Saved: processed_images\\kanchan_WhatsApp Image 2025-06-08 at 12.18.31_da1b067b.jpg\n",
      "Reading: C:\\Users\\kanch\\OneDrive\\Desktop\\face recognition system\\images\\kanchan\\WhatsApp Image 2025-06-08 at 12.23.42_93f0ef31.jpg\n",
      "Saved: processed_images\\kanchan_WhatsApp Image 2025-06-08 at 12.23.42_93f0ef31.jpg\n",
      "Reading: C:\\Users\\kanch\\OneDrive\\Desktop\\face recognition system\\images\\kanchan\\WhatsApp Image 2025-06-08 at 12.23.43_ed62a114.jpg\n",
      "Saved: processed_images\\kanchan_WhatsApp Image 2025-06-08 at 12.23.43_ed62a114.jpg\n",
      "Reading: C:\\Users\\kanch\\OneDrive\\Desktop\\face recognition system\\images\\kanchan\\WhatsApp Image 2025-06-08 at 12.23.44_d0b5d6d0.jpg\n",
      "Saved: processed_images\\kanchan_WhatsApp Image 2025-06-08 at 12.23.44_d0b5d6d0.jpg\n",
      "Reading: C:\\Users\\kanch\\OneDrive\\Desktop\\face recognition system\\images\\kanchan\\WhatsApp Image 2025-06-08 at 12.23.45_8c0d3ada.jpg\n",
      "Saved: processed_images\\kanchan_WhatsApp Image 2025-06-08 at 12.23.45_8c0d3ada.jpg\n",
      "Reading: C:\\Users\\kanch\\OneDrive\\Desktop\\face recognition system\\images\\kanchan\\WhatsApp Image 2025-06-08 at 12.23.45_b90a2bb8.jpg\n",
      "Saved: processed_images\\kanchan_WhatsApp Image 2025-06-08 at 12.23.45_b90a2bb8.jpg\n",
      "Reading: C:\\Users\\kanch\\OneDrive\\Desktop\\face recognition system\\images\\kanchan\\WhatsApp Image 2025-06-08 at 12.23.46_c6135fa1.jpg\n",
      "Saved: processed_images\\kanchan_WhatsApp Image 2025-06-08 at 12.23.46_c6135fa1.jpg\n",
      "Reading: C:\\Users\\kanch\\OneDrive\\Desktop\\face recognition system\\images\\kanchan\\WhatsApp Image 2025-06-08 at 12.23.47_54ce534a.jpg\n",
      "Saved: processed_images\\kanchan_WhatsApp Image 2025-06-08 at 12.23.47_54ce534a.jpg\n",
      "Reading: C:\\Users\\kanch\\OneDrive\\Desktop\\face recognition system\\images\\muskan\\WhatsApp Image 2025-06-08 at 12.18.32_243d8c3f.jpg\n",
      "Saved: processed_images\\muskan_WhatsApp Image 2025-06-08 at 12.18.32_243d8c3f.jpg\n",
      "Reading: C:\\Users\\kanch\\OneDrive\\Desktop\\face recognition system\\images\\muskan\\WhatsApp Image 2025-06-08 at 12.18.33_5d28704e.jpg\n",
      "Saved: processed_images\\muskan_WhatsApp Image 2025-06-08 at 12.18.33_5d28704e.jpg\n",
      "Reading: C:\\Users\\kanch\\OneDrive\\Desktop\\face recognition system\\images\\muskan\\WhatsApp Image 2025-06-08 at 12.18.35_b2d0b4b7.jpg\n",
      "Saved: processed_images\\muskan_WhatsApp Image 2025-06-08 at 12.18.35_b2d0b4b7.jpg\n",
      "Reading: C:\\Users\\kanch\\OneDrive\\Desktop\\face recognition system\\images\\muskan\\WhatsApp Image 2025-06-08 at 12.18.36_2164f4cc.jpg\n",
      "Saved: processed_images\\muskan_WhatsApp Image 2025-06-08 at 12.18.36_2164f4cc.jpg\n",
      "Reading: C:\\Users\\kanch\\OneDrive\\Desktop\\face recognition system\\images\\muskan\\WhatsApp Image 2025-06-08 at 12.23.39_ea306db1.jpg\n",
      "Saved: processed_images\\muskan_WhatsApp Image 2025-06-08 at 12.23.39_ea306db1.jpg\n",
      "Reading: C:\\Users\\kanch\\OneDrive\\Desktop\\face recognition system\\images\\muskan\\WhatsApp Image 2025-06-08 at 12.23.40_e7749b64.jpg\n",
      "Saved: processed_images\\muskan_WhatsApp Image 2025-06-08 at 12.23.40_e7749b64.jpg\n",
      "Reading: C:\\Users\\kanch\\OneDrive\\Desktop\\face recognition system\\images\\muskan\\WhatsApp Image 2025-06-08 at 12.23.41_c8e2bfe4.jpg\n",
      "Saved: processed_images\\muskan_WhatsApp Image 2025-06-08 at 12.23.41_c8e2bfe4.jpg\n",
      "Reading: C:\\Users\\kanch\\OneDrive\\Desktop\\face recognition system\\images\\muskan\\WhatsApp Image 2025-06-08 at 12.23.41_f3cb1f8b.jpg\n",
      "Saved: processed_images\\muskan_WhatsApp Image 2025-06-08 at 12.23.41_f3cb1f8b.jpg\n",
      "Reading: C:\\Users\\kanch\\OneDrive\\Desktop\\face recognition system\\images\\muskan\\WhatsApp Image 2025-06-08 at 12.23.42_c3886570.jpg\n",
      "Saved: processed_images\\muskan_WhatsApp Image 2025-06-08 at 12.23.42_c3886570.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "input_folder = r\"C:\\Users\\kanch\\OneDrive\\Desktop\\face recognition system\\images\"  # The folder that contains 'person1'\n",
    "output_folder = \"processed_images\"\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Traverse subfolders\n",
    "for subfolder in os.listdir(input_folder):\n",
    "    subfolder_path = os.path.join(input_folder, subfolder)\n",
    "\n",
    "    # Ensure it's a folder\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        for filename in os.listdir(subfolder_path):\n",
    "            if filename.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):\n",
    "                image_path = os.path.join(subfolder_path, filename)\n",
    "                print(f\"Reading: {image_path}\")\n",
    "\n",
    "                image = cv2.imread(image_path)\n",
    "\n",
    "                if image is None:\n",
    "                    print(f\"Image couldn't be read: {filename}\")\n",
    "                    continue\n",
    "\n",
    "                # Preprocess (e.g., convert to grayscale)\n",
    "                gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                # Save to output folder with unique name\n",
    "                save_path = os.path.join(output_folder, f\"{subfolder}_{filename}\")\n",
    "                success = cv2.imwrite(save_path, gray)\n",
    "\n",
    "                if success:\n",
    "                    print(f\"Saved: {save_path}\")\n",
    "                else:\n",
    "                    print(f\"Failed to save: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b94ac198-6f6c-447b-a2fe-038ffe056e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in folder: ['kanchan', 'muskan']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_folder = r\"C:\\Users\\kanch\\OneDrive\\Desktop\\face recognition system\\images\"\n",
    "print(\"Files in folder:\", os.listdir(input_folder))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02aa8a6d-5591-4b34-92c9-2b876048ccc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-facenet in c:\\users\\kanch\\facenet_env\\lib\\site-packages (0.3.2)\n",
      "Requirement already satisfied: mtcnn in c:\\users\\kanch\\facenet_env\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\kanch\\facenet_env\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: lz4>=4.3.3 in c:\\users\\kanch\\facenet_env\\lib\\site-packages (from mtcnn) (4.4.4)\n",
      "Requirement already satisfied: joblib>=1.4.2 in c:\\users\\kanch\\facenet_env\\lib\\site-packages (from mtcnn) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.3 in c:\\users\\kanch\\facenet_env\\lib\\site-packages (from opencv-python) (1.24.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\kanch\\facenet_env\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-facenet mtcnn opencv-python\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4391d169-93af-4780-8bfa-1dbec472ddc1",
   "metadata": {},
   "source": [
    "#face encoding and features extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5383146-3132-4617-b2ec-9757d6ceceee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: 'C:\\\\Users\\\\kanch\\\\OneDrive\\\\Desktop\\\\face'\n",
      "Hint: It looks like a path. File 'C:\\Users\\kanch\\OneDrive\\Desktop\\face' does not exist.\n",
      "WARNING: You are using pip version 21.2.3; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\kanch\\facenet_env\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install C:\\Users\\kanch\\OneDrive\\Desktop\\face recognition system\\dlib-19.24.99-cp312-cp312-win_amd64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7771c48f-713e-4dca-b410-835b7e6574e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: C:\\Users\\kanch\\OneDrive\\Desktop\\face recognition system\\images\\kanchan\\WhatsApp Image 2025-06-08 at 12.18.28_1ef9c98c.jpg\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "Processing: C:\\Users\\kanch\\OneDrive\\Desktop\\face recognition system\\images\\kanchan\\WhatsApp Image 2025-06-08 at 12.18.29_31a2bba0.jpg\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "Processing: C:\\Users\\kanch\\OneDrive\\Desktop\\face recognition system\\images\\kanchan\\WhatsApp Image 2025-06-08 at 12.18.30_625eb08d.jpg\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "Processing: C:\\Users\\kanch\\OneDrive\\Desktop\\face recognition system\\images\\kanchan\\WhatsApp Image 2025-06-08 at 12.18.31_da1b067b.jpg\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "Processing: C:\\Users\\kanch\\OneDrive\\Desktop\\face recognition system\\images\\kanchan\\WhatsApp Image 2025-06-08 at 12.23.42_93f0ef31.jpg\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "Processing: C:\\Users\\kanch\\OneDrive\\Desktop\\face recognition system\\images\\kanchan\\WhatsApp Image 2025-06-08 at 12.23.43_ed62a114.jpg\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "Processing: C:\\Users\\kanch\\OneDrive\\Desktop\\face recognition system\\images\\kanchan\\WhatsApp Image 2025-06-08 at 12.23.44_d0b5d6d0.jpg\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "Processing: C:\\Users\\kanch\\OneDrive\\Desktop\\face recognition system\\images\\kanchan\\WhatsApp Image 2025-06-08 at 12.23.45_8c0d3ada.jpg\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "Processing: C:\\Users\\kanch\\OneDrive\\Desktop\\face recognition system\\images\\kanchan\\WhatsApp Image 2025-06-08 at 12.23.45_b90a2bb8.jpg\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "Processing: C:\\Users\\kanch\\OneDrive\\Desktop\\face recognition system\\images\\kanchan\\WhatsApp Image 2025-06-08 at 12.23.46_c6135fa1.jpg\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "Processing: C:\\Users\\kanch\\OneDrive\\Desktop\\face recognition system\\images\\kanchan\\WhatsApp Image 2025-06-08 at 12.23.47_54ce534a.jpg\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "Processing: C:\\Users\\kanch\\OneDrive\\Desktop\\face recognition system\\images\\muskan\\WhatsApp Image 2025-06-08 at 12.18.32_243d8c3f.jpg\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "Processing: C:\\Users\\kanch\\OneDrive\\Desktop\\face recognition system\\images\\muskan\\WhatsApp Image 2025-06-08 at 12.18.33_5d28704e.jpg\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "Processing: C:\\Users\\kanch\\OneDrive\\Desktop\\face recognition system\\images\\muskan\\WhatsApp Image 2025-06-08 at 12.18.35_b2d0b4b7.jpg\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "Processing: C:\\Users\\kanch\\OneDrive\\Desktop\\face recognition system\\images\\muskan\\WhatsApp Image 2025-06-08 at 12.18.36_2164f4cc.jpg\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "Processing: C:\\Users\\kanch\\OneDrive\\Desktop\\face recognition system\\images\\muskan\\WhatsApp Image 2025-06-08 at 12.23.39_ea306db1.jpg\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "Processing: C:\\Users\\kanch\\OneDrive\\Desktop\\face recognition system\\images\\muskan\\WhatsApp Image 2025-06-08 at 12.23.40_e7749b64.jpg\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "Processing: C:\\Users\\kanch\\OneDrive\\Desktop\\face recognition system\\images\\muskan\\WhatsApp Image 2025-06-08 at 12.23.41_c8e2bfe4.jpg\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "Processing: C:\\Users\\kanch\\OneDrive\\Desktop\\face recognition system\\images\\muskan\\WhatsApp Image 2025-06-08 at 12.23.41_f3cb1f8b.jpg\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "Processing: C:\\Users\\kanch\\OneDrive\\Desktop\\face recognition system\\images\\muskan\\WhatsApp Image 2025-06-08 at 12.23.42_c3886570.jpg\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "Total encodings extracted: 20\n"
     ]
    }
   ],
   "source": [
    "from keras_facenet import FaceNet\n",
    "from mtcnn import MTCNN\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Initialize FaceNet and MTCNN\n",
    "embedder = FaceNet()\n",
    "detector = MTCNN()\n",
    "\n",
    "# Path to original dataset (subfolders = labels)\n",
    "input_folder = r\"C:\\Users\\kanch\\OneDrive\\Desktop\\face recognition system\\images\"\n",
    "\n",
    "embeddings = []\n",
    "names = []\n",
    "\n",
    "# Loop over people\n",
    "for person_name in os.listdir(input_folder):\n",
    "    person_folder = os.path.join(input_folder, person_name)\n",
    "\n",
    "    if os.path.isdir(person_folder):\n",
    "        for filename in os.listdir(person_folder):\n",
    "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                image_path = os.path.join(person_folder, filename)\n",
    "                print(f\"Processing: {image_path}\")\n",
    "\n",
    "                image = cv2.imread(image_path)\n",
    "                image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                # Detect face\n",
    "                results = detector.detect_faces(image_rgb)\n",
    "\n",
    "                if results:\n",
    "                    x, y, w, h = results[0]['box']\n",
    "                    face = image_rgb[y:y+h, x:x+w]\n",
    "                    face = cv2.resize(face, (160, 160))\n",
    "                    face = np.asarray(face)\n",
    "\n",
    "                    # Generate embedding\n",
    "                    embedding = embedder.embeddings([face])[0]  # 128D\n",
    "                    embeddings.append(embedding)\n",
    "                    names.append(person_name)\n",
    "                else:\n",
    "                    print(\"No face found in:\", filename)\n",
    "\n",
    "print(f\"Total encodings extracted: {len(embeddings)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e681559-f30f-4ff4-b12b-cb6c250035ad",
   "metadata": {},
   "source": [
    "train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2adf6b70-0505-47d5-ab40-9a47baafa68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: facenet-pytorch in c:\\users\\kanch\\facenet_env\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: torch<2.3.0,>=2.2.0 in c:\\users\\kanch\\facenet_env\\lib\\site-packages (from facenet-pytorch) (2.2.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\kanch\\facenet_env\\lib\\site-packages (from facenet-pytorch) (2.32.3)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.24.0 in c:\\users\\kanch\\facenet_env\\lib\\site-packages (from facenet-pytorch) (1.24.3)\n",
      "Requirement already satisfied: torchvision<0.18.0,>=0.17.0 in c:\\users\\kanch\\facenet_env\\lib\\site-packages (from facenet-pytorch) (0.17.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.0.0 in c:\\users\\kanch\\facenet_env\\lib\\site-packages (from facenet-pytorch) (4.67.1)\n",
      "Requirement already satisfied: Pillow<10.3.0,>=10.2.0 in c:\\users\\kanch\\facenet_env\\lib\\site-packages (from facenet-pytorch) (10.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kanch\\facenet_env\\lib\\site-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kanch\\facenet_env\\lib\\site-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kanch\\facenet_env\\lib\\site-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kanch\\facenet_env\\lib\\site-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2025.1.31)\n",
      "Requirement already satisfied: networkx in c:\\users\\kanch\\facenet_env\\lib\\site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.4.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\kanch\\facenet_env\\lib\\site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (1.13.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kanch\\facenet_env\\lib\\site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.1.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\kanch\\facenet_env\\lib\\site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.18.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\kanch\\facenet_env\\lib\\site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\kanch\\facenet_env\\lib\\site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (4.13.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\kanch\\facenet_env\\lib\\site-packages (from tqdm<5.0.0,>=4.0.0->facenet-pytorch) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kanch\\facenet_env\\lib\\site-packages (from jinja2->torch<2.3.0,>=2.2.0->facenet-pytorch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\kanch\\facenet_env\\lib\\site-packages (from sympy->torch<2.3.0,>=2.2.0->facenet-pytorch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\kanch\\facenet_env\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install facenet-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5c04928-f5ce-4319-ac65-c6d672fe99fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "32/32 [==============================] - 1s 2ms/step - loss: 0.7083 - accuracy: 0.5110\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6986 - accuracy: 0.5260\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6900 - accuracy: 0.5400\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6849 - accuracy: 0.5410\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6805 - accuracy: 0.5600\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6761 - accuracy: 0.5680\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6685 - accuracy: 0.5950\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6659 - accuracy: 0.5920\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6584 - accuracy: 0.6210\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6561 - accuracy: 0.6180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanch\\facenet_env\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# model.save(\"facenet_classifier_model.h5\")\n",
    "\n",
    "# # Save encoder\n",
    "# with open(\"label_encoder.pkl\", \"wb\") as f:\n",
    "#    pickle.dump(encoder, f)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Example model definition\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Example model compilation and training\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Example training data (replace with your actual data)\n",
    "import numpy as np\n",
    "data = np.random.random((1000, 100))\n",
    "labels = np.random.randint(2, size=(1000, 1))\n",
    "\n",
    "# Example model training\n",
    "model.fit(data, labels, epochs=10, batch_size=32)\n",
    "\n",
    "# Save the model after training\n",
    "model.save(\"facenet_classifier_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87e6e313-f2ed-485a-8b6e-a858336e0b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n",
      "Epoch 1/25\n",
      "2/2 [==============================] - 1s 309ms/step - loss: 0.7071 - accuracy: 0.3750 - val_loss: 0.6963 - val_accuracy: 0.5000\n",
      "Epoch 2/25\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.6947 - accuracy: 0.5625 - val_loss: 0.6983 - val_accuracy: 0.5000\n",
      "Epoch 3/25\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.6945 - accuracy: 0.5625 - val_loss: 0.6986 - val_accuracy: 0.5000\n",
      "Epoch 4/25\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.6776 - accuracy: 0.5625 - val_loss: 0.6990 - val_accuracy: 0.5000\n",
      "Epoch 5/25\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.6929 - accuracy: 0.5625 - val_loss: 0.6995 - val_accuracy: 0.5000\n",
      "Epoch 6/25\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6893 - accuracy: 0.5625 - val_loss: 0.6993 - val_accuracy: 0.5000\n",
      "Epoch 7/25\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.6843 - accuracy: 0.5625 - val_loss: 0.7010 - val_accuracy: 0.5000\n",
      "Epoch 8/25\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.6938 - accuracy: 0.5625 - val_loss: 0.7014 - val_accuracy: 0.5000\n",
      "Epoch 9/25\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6971 - accuracy: 0.5625 - val_loss: 0.7006 - val_accuracy: 0.5000\n",
      "Epoch 10/25\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.6831 - accuracy: 0.5625 - val_loss: 0.7002 - val_accuracy: 0.5000\n",
      "Epoch 11/25\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6720 - accuracy: 0.5625 - val_loss: 0.7008 - val_accuracy: 0.5000\n",
      "Epoch 12/25\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.6912 - accuracy: 0.5625 - val_loss: 0.7011 - val_accuracy: 0.5000\n",
      "Epoch 13/25\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.6977 - accuracy: 0.5625 - val_loss: 0.7020 - val_accuracy: 0.5000\n",
      "Epoch 14/25\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.6944 - accuracy: 0.5625 - val_loss: 0.7015 - val_accuracy: 0.5000\n",
      "Epoch 15/25\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.6834 - accuracy: 0.5625 - val_loss: 0.7016 - val_accuracy: 0.5000\n",
      "Epoch 16/25\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6850 - accuracy: 0.5625 - val_loss: 0.7021 - val_accuracy: 0.5000\n",
      "Epoch 17/25\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.6691 - accuracy: 0.5625 - val_loss: 0.7029 - val_accuracy: 0.5000\n",
      "Epoch 18/25\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.6756 - accuracy: 0.5625 - val_loss: 0.7030 - val_accuracy: 0.5000\n",
      "Epoch 19/25\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.6796 - accuracy: 0.5625 - val_loss: 0.7037 - val_accuracy: 0.5000\n",
      "Epoch 20/25\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6853 - accuracy: 0.5625 - val_loss: 0.7033 - val_accuracy: 0.5000\n",
      "Epoch 21/25\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.6926 - accuracy: 0.5625 - val_loss: 0.7035 - val_accuracy: 0.5000\n",
      "Epoch 22/25\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.6869 - accuracy: 0.5625 - val_loss: 0.7045 - val_accuracy: 0.5000\n",
      "Epoch 23/25\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.6865 - accuracy: 0.5625 - val_loss: 0.7046 - val_accuracy: 0.5000\n",
      "Epoch 24/25\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.7046 - accuracy: 0.5625 - val_loss: 0.7044 - val_accuracy: 0.5000\n",
      "Epoch 25/25\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.6871 - accuracy: 0.5625 - val_loss: 0.7044 - val_accuracy: 0.5000\n",
      "Model trained and saved successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanch\\facenet_env\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras_facenet import FaceNet\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import cv2\n",
    "import pickle\n",
    "\n",
    "# Load embedder\n",
    "embedder = FaceNet()\n",
    "\n",
    "# Set the dataset directory (change to your actual image folder)\n",
    "dataset_path = r\"C:\\Users\\kanch\\OneDrive\\Desktop\\face recognition system\\images\"\n",
    "\n",
    "faces = []\n",
    "labels = []\n",
    "\n",
    "# Read images and labels\n",
    "for person_name in os.listdir(dataset_path):\n",
    "    person_dir = os.path.join(dataset_path, person_name)\n",
    "    if not os.path.isdir(person_dir):\n",
    "        continue\n",
    "\n",
    "    for filename in os.listdir(person_dir):\n",
    "        img_path = os.path.join(person_dir, filename)\n",
    "        if not filename.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):\n",
    "            continue\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            print(f\"Failed to read: {img_path}\")\n",
    "            continue\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (160, 160))\n",
    "        img = img.astype('float32') / 255.0\n",
    "        faces.append(img)\n",
    "        labels.append(person_name)\n",
    "\n",
    "# Convert to embeddings\n",
    "embeddings = embedder.embeddings(faces)\n",
    "\n",
    "# Encode labels\n",
    "encoder = LabelEncoder()\n",
    "encoded_labels = encoder.fit_transform(labels)\n",
    "y = to_categorical(encoded_labels)\n",
    "\n",
    "# Save label encoder\n",
    "with open(\"label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(encoder, f)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build classifier\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(512,)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(len(np.unique(labels)), activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=25, batch_size=8, validation_data=(X_test, y_test))\n",
    "\n",
    "# Save model\n",
    "model.save(\"facenet_classifier_model.h5\")\n",
    "\n",
    "print(\"Model trained and saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fae9d70-2c2b-4e64-a527-e8036a68bbde",
   "metadata": {},
   "source": [
    "test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a1d6637-9b55-474f-9b30-28135280eae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Reading images and detecting faces...\n",
      "[INFO] Total faces loaded: 20\n",
      "[INFO] Generating embeddings...\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "[INFO] Embedding shape: (20, 512)\n",
      "[INFO] Building and training model...\n",
      "Epoch 1/30\n",
      "2/2 [==============================] - 1s 342ms/step - loss: 0.6895 - accuracy: 0.4375 - val_loss: 0.5913 - val_accuracy: 1.0000\n",
      "Epoch 2/30\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.5911 - accuracy: 1.0000 - val_loss: 0.5245 - val_accuracy: 1.0000\n",
      "Epoch 3/30\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.5015 - accuracy: 1.0000 - val_loss: 0.4640 - val_accuracy: 1.0000\n",
      "Epoch 4/30\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.4307 - accuracy: 1.0000 - val_loss: 0.4045 - val_accuracy: 1.0000\n",
      "Epoch 5/30\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.3622 - accuracy: 1.0000 - val_loss: 0.3431 - val_accuracy: 1.0000\n",
      "Epoch 6/30\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.2981 - accuracy: 1.0000 - val_loss: 0.2845 - val_accuracy: 1.0000\n",
      "Epoch 7/30\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.2404 - accuracy: 1.0000 - val_loss: 0.2317 - val_accuracy: 1.0000\n",
      "Epoch 8/30\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.1894 - accuracy: 1.0000 - val_loss: 0.1856 - val_accuracy: 1.0000\n",
      "Epoch 9/30\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.1483 - accuracy: 1.0000 - val_loss: 0.1466 - val_accuracy: 1.0000\n",
      "Epoch 10/30\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.1159 - accuracy: 1.0000 - val_loss: 0.1147 - val_accuracy: 1.0000\n",
      "Epoch 11/30\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.0885 - accuracy: 1.0000 - val_loss: 0.0891 - val_accuracy: 1.0000\n",
      "Epoch 12/30\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.0679 - accuracy: 1.0000 - val_loss: 0.0690 - val_accuracy: 1.0000\n",
      "Epoch 13/30\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.0530 - accuracy: 1.0000 - val_loss: 0.0536 - val_accuracy: 1.0000\n",
      "Epoch 14/30\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.0410 - accuracy: 1.0000 - val_loss: 0.0421 - val_accuracy: 1.0000\n",
      "Epoch 15/30\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.0328 - accuracy: 1.0000 - val_loss: 0.0333 - val_accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.0256 - accuracy: 1.0000 - val_loss: 0.0267 - val_accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.0210 - accuracy: 1.0000 - val_loss: 0.0217 - val_accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.0171 - accuracy: 1.0000 - val_loss: 0.0179 - val_accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.0143 - accuracy: 1.0000 - val_loss: 0.0150 - val_accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.0120 - accuracy: 1.0000 - val_loss: 0.0127 - val_accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.0109 - val_accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.0095 - val_accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.0084 - val_accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.0075 - val_accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.0067 - val_accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.0061 - val_accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.0056 - val_accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.0051 - val_accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.0048 - val_accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.0045 - val_accuracy: 1.0000\n",
      "[INFO] Model saved to facenet_classifier_model.h5\n",
      "[INFO] Label encoder saved to label_encoder.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanch\\facenet_env\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "from mtcnn import MTCNN\n",
    "from keras_facenet import FaceNet\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Paths\n",
    "dataset_path = r\"C:\\Users\\kanch\\OneDrive\\Desktop\\face recognition system\\images\"\n",
    "model_path = \"facenet_classifier_model.h5\"\n",
    "encoder_path = \"label_encoder.pkl\"\n",
    "\n",
    "# Load FaceNet embedder and face detector\n",
    "embedder = FaceNet()\n",
    "detector = MTCNN()\n",
    "\n",
    "faces = []\n",
    "labels = []\n",
    "\n",
    "# Traverse dataset directory\n",
    "print(\"[INFO] Reading images and detecting faces...\")\n",
    "for label_name in os.listdir(dataset_path):\n",
    "    label_path = os.path.join(dataset_path, label_name)\n",
    "    if not os.path.isdir(label_path):\n",
    "        continue\n",
    "\n",
    "    for img_name in os.listdir(label_path):\n",
    "        img_path = os.path.join(label_path, img_name)\n",
    "\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None or img.shape[0] == 0 or img.shape[1] == 0:\n",
    "            print(f\"[WARNING] Skipping unreadable or invalid image: {img_path}\")\n",
    "            continue\n",
    "\n",
    "        rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        results = detector.detect_faces(rgb_img)\n",
    "\n",
    "        if results:\n",
    "            x, y, w, h = results[0]['box']\n",
    "            x, y = max(0, x), max(0, y)\n",
    "            face = rgb_img[y:y+h, x:x+w]\n",
    "            try:\n",
    "                face = cv2.resize(face, (160, 160))\n",
    "                faces.append(face)\n",
    "                labels.append(label_name)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARNING] Error resizing face: {img_path} | {str(e)}\")\n",
    "        else:\n",
    "            print(f\"[INFO] No face detected in: {img_path}\")\n",
    "\n",
    "print(f\"[INFO] Total faces loaded: {len(faces)}\")\n",
    "\n",
    "if len(faces) == 0:\n",
    "    raise ValueError(\"No valid faces found in the dataset. Please check your dataset structure and image files.\")\n",
    "\n",
    "# Get embeddings\n",
    "print(\"[INFO] Generating embeddings...\")\n",
    "embeddings = embedder.embeddings(faces)\n",
    "\n",
    "# Print the shape of embeddings to confirm\n",
    "print(f\"[INFO] Embedding shape: {embeddings.shape}\")\n",
    "\n",
    "# Encode labels\n",
    "encoder = LabelEncoder()\n",
    "encoded_labels = encoder.fit_transform(labels)\n",
    "categorical_labels = to_categorical(encoded_labels)\n",
    "\n",
    "# Save the encoder\n",
    "with open(encoder_path, \"wb\") as f:\n",
    "    pickle.dump(encoder, f)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    embeddings, categorical_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build classifier model\n",
    "print(\"[INFO] Building and training model...\")\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_dim=512, activation='relu'))  # Ensure input shape matches embedding dimension\n",
    "model.add(Dense(len(encoder.classes_), activation='softmax'))  # Output layer with number of classes\n",
    "model.compile(optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(np.array(X_train), np.array(y_train), epochs=30, batch_size=8, validation_data=(X_test, y_test))\n",
    "\n",
    "# Save model\n",
    "model.save(model_path)\n",
    "print(f\"[INFO] Model saved to {model_path}\")\n",
    "print(f\"[INFO] Label encoder saved to {encoder_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "659ba0af-5571-440b-8363-28a042242951",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 495-496: truncated \\UXXXXXXXX escape (2426251990.py, line 68)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[11], line 68\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(\"No face found in the image.\")'''\u001b[0m\n\u001b[1;37m                                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 495-496: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "'''import cv2\n",
    "import numpy as np\n",
    "from keras_facenet import FaceNet\n",
    "from mtcnn import MTCNN\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "\n",
    "# Load the trained classifier model\n",
    "model = load_model(\"facenet_classifier_model.h5\")\n",
    "\n",
    "# Load the label encoder\n",
    "with open(\"label_encoder.pkl\", \"rb\") as file:\n",
    "    encoder = pickle.load(file)\n",
    "\n",
    "# Initialize FaceNet embedder and MTCNN face detector\n",
    "embedder = FaceNet()\n",
    "detector = MTCNN()\n",
    "\n",
    "# Load and preprocess the test image\n",
    "test_image_path = r\"C:\\Users\\hp\\Downloads\\Test.jpg\"\n",
    "image = cv2.imread(test_image_path)\n",
    "rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Detect faces\n",
    "results = detector.detect_faces(rgb_image)\n",
    "\n",
    "if results:\n",
    "    for result in results:\n",
    "        # Extract bounding box\n",
    "        x, y, w, h = result['box']\n",
    "\n",
    "        # Crop and resize the face\n",
    "        face = rgb_image[y:y+h, x:x+w]\n",
    "        face = cv2.resize(face, (160, 160))\n",
    "\n",
    "        # Get the FaceNet embedding (shape: (512,))\n",
    "        embedding = embedder.embeddings([face])[0]\n",
    "\n",
    "        # Expand dimensions to (1, 512) for prediction\n",
    "        embedding = np.expand_dims(embedding, axis=0)\n",
    "\n",
    "        # Make prediction\n",
    "        prediction = model.predict(embedding, verbose=0)[0]\n",
    "\n",
    "        # Get predicted label and confidence\n",
    "        predicted_index = np.argmax(prediction)\n",
    "        predicted_label = encoder.inverse_transform([predicted_index])[0]\n",
    "        confidence = np.max(prediction)\n",
    "\n",
    "        # Draw bounding box and label on the image\n",
    "        cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        cv2.putText(\n",
    "            image, \n",
    "            f\"{predicted_label} ({confidence*100:.1f}%)\", \n",
    "            (x, y-10), \n",
    "            cv2.FONT_HERSHEY_SIMPLEX, \n",
    "            0.8, \n",
    "            (255, 0, 0), \n",
    "            2\n",
    "        )\n",
    "\n",
    "    # Show the image with predictions\n",
    "    cv2.imshow(\"Prediction\", image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "else:\n",
    "    print(\"No face found in the image.\")'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7897ca8e-8135-4747-8e05-dc1e8aaafb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System initialized. Press 'q' to quit.\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "WARNING:tensorflow:5 out of the last 24 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001BC50E93BE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "MATCH FOUND: kanchan with confidence 99.42%\n",
      "[SPEAK]: Match found. Hello kanchan\n",
      " Door opened.\n",
      "[SPEAK]: Door opened\n",
      "Door closed.\n",
      "[SPEAK]: Door closed\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "MATCH FOUND: kanchan with confidence 99.71%\n",
      "[SPEAK]: Match found. Hello kanchan\n",
      " Door opened.\n",
      "[SPEAK]: Door opened\n",
      "Door closed.\n",
      "[SPEAK]: Door closed\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "MATCH FOUND: kanchan with confidence 99.75%\n",
      "[SPEAK]: Match found. Hello kanchan\n",
      " Door opened.\n",
      "[SPEAK]: Door opened\n",
      "Door closed.\n",
      "[SPEAK]: Door closed\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "MATCH FOUND: kanchan with confidence 98.74%\n",
      "[SPEAK]: Match found. Hello kanchan\n",
      " Door opened.\n",
      "[SPEAK]: Door opened\n",
      "Door closed.\n",
      "[SPEAK]: Door closed\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "MATCH FOUND: kanchan with confidence 95.31%\n",
      "[SPEAK]: Match found. Hello kanchan\n",
      " Door opened.\n",
      "[SPEAK]: Door opened\n",
      "Door closed.\n",
      "[SPEAK]: Door closed\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "MATCH FOUND: kanchan with confidence 96.71%\n",
      "[SPEAK]: Match found. Hello kanchan\n",
      " Door opened.\n",
      "[SPEAK]: Door opened\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras_facenet import FaceNet\n",
    "from mtcnn import MTCNN\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "import time\n",
    "import pyttsx3\n",
    "\n",
    "# Voice setup\n",
    "engine = pyttsx3.init()\n",
    "def speak(text):\n",
    "    print(\"[SPEAK]:\", text)\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "\n",
    "# Load model and encoder\n",
    "model = load_model(\"facenet_classifier_model.h5\")\n",
    "with open(\"label_encoder.pkl\", \"rb\") as f:\n",
    "    encoder = pickle.load(f)\n",
    "\n",
    "# Initialize\n",
    "detector = MTCNN()\n",
    "embedder = FaceNet()\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 500)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 500)\n",
    "\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "# Settings\n",
    "match_interval = 15  # seconds between match attempts\n",
    "last_match_time = 0\n",
    "success_count = 0\n",
    "fail_count = 0\n",
    "max_fail = 3\n",
    "sleep_timeout = 10  # seconds of no face -> sleep\n",
    "last_seen_time = time.time()\n",
    "\n",
    "frame_status = \"blue\"\n",
    "predicted_label = \"\"\n",
    "confidence_percent = 0\n",
    "\n",
    "door_status = \"\"\n",
    "door_timer = 0\n",
    "door_opening = False\n",
    "match_found_time = 0\n",
    "process_started = False\n",
    "auth_in_progress = False\n",
    "sleeping = False\n",
    "\n",
    "def sleeping_mode_display():\n",
    "    black_frame = np.zeros((500, 500, 3), dtype=np.uint8)\n",
    "    cv2.putText(black_frame, \"SLEEP MODE\", (100, 250), font, 1.2, (255, 255, 255), 3)\n",
    "    cv2.imshow(\"Face Recognition System\", black_frame)\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "def waking_up_display():\n",
    "    wake_frame = np.zeros((500, 500, 3), dtype=np.uint8)\n",
    "    cv2.putText(wake_frame, \"WAKING...\", (130, 250), font, 1.2, (0, 255, 0), 3)\n",
    "    for _ in range(20):  # Show for ~2 seconds\n",
    "        cv2.imshow(\"Face Recognition System\", wake_frame)\n",
    "        cv2.waitKey(100)\n",
    "\n",
    "print(\"System initialized. Press 'q' to quit.\")\n",
    "\n",
    "while True:\n",
    "    if sleeping:\n",
    "        sleeping_mode_display()\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            print(\"Exiting system...\")\n",
    "            break\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = detector.detect_faces(rgb_frame)\n",
    "        if results:\n",
    "            print(\"Face detected. Waking up system...\")\n",
    "            speak(\"Waking up\")\n",
    "            waking_up_display()\n",
    "            sleeping = False\n",
    "            last_seen_time = time.time()\n",
    "        continue\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    current_time = time.time()\n",
    "\n",
    "    # Face detection\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = detector.detect_faces(rgb_frame)\n",
    "\n",
    "    if results:\n",
    "        last_seen_time = current_time\n",
    "    elif not sleeping and (current_time - last_seen_time > sleep_timeout):\n",
    "        print(\"No face detected. Entering sleep mode...\")\n",
    "        speak(\"System sleeping\")\n",
    "        sleeping = True\n",
    "        continue\n",
    "\n",
    "    if results and not auth_in_progress and (current_time - last_match_time > match_interval):\n",
    "        x, y, w_box, h_box = results[0]['box']\n",
    "        x, y = max(0, x), max(0, y)\n",
    "        face = rgb_frame[y:y + h_box, x:x + w_box]\n",
    "        try:\n",
    "            face = cv2.resize(face, (160, 160))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        embedding = embedder.embeddings([face])[0]\n",
    "        prediction = model.predict(np.expand_dims(embedding, axis=0))[0]\n",
    "        confidence = np.max(prediction)\n",
    "\n",
    "        label_index = np.argmax(prediction)\n",
    "        try:\n",
    "            predicted_label = encoder.inverse_transform([label_index])[0]\n",
    "        except:\n",
    "            predicted_label = f\"Unknown (index {label_index})\"\n",
    "\n",
    "        confidence_percent = confidence * 100\n",
    "        last_match_time = current_time\n",
    "        auth_in_progress = True\n",
    "\n",
    "        if confidence > 0.8:\n",
    "            print(f\"MATCH FOUND: {predicted_label} with confidence {confidence_percent:.2f}%\")\n",
    "            speak(f\"Match found. Hello {predicted_label}\")\n",
    "            frame_status = \"green\"\n",
    "            match_found_time = current_time\n",
    "            door_status = \"Match Found\"\n",
    "            process_started = True\n",
    "            success_count += 1\n",
    "            fail_count = 0\n",
    "        else:\n",
    "            print(f\" MATCH NOT FOUND. Confidence: {confidence_percent:.2f}%\")\n",
    "            fail_count += 1\n",
    "            speak(\"Match not found\")\n",
    "            frame_status = \"red\"\n",
    "            if fail_count >= max_fail:\n",
    "                print(\" Maximum unauthorized attempts reached. Shutting down.\")\n",
    "                speak(\"Maximum unauth reached\")\n",
    "                break\n",
    "            auth_in_progress = False\n",
    "\n",
    "    # Door control logic\n",
    "    if process_started:\n",
    "        elapsed = current_time - match_found_time\n",
    "        if elapsed >= 1 and not door_opening:\n",
    "            print(\" Door opened.\")\n",
    "            speak(\"Door opened\")\n",
    "            door_status = \"Door Opened\"\n",
    "            door_opening = True\n",
    "            door_timer = current_time\n",
    "\n",
    "        if door_opening and current_time - door_timer >= 10:\n",
    "            print(\"Door closed.\")\n",
    "            speak(\"Door closed\")\n",
    "            door_status = \"Door Closed\"\n",
    "            frame_status = \"blue\"\n",
    "            door_opening = False\n",
    "            process_started = False\n",
    "            auth_in_progress = False\n",
    "\n",
    "    # Draw rectangle frame\n",
    "    h, w = frame.shape[:2]\n",
    "    if frame_status == \"green\":\n",
    "        color = (0, 255, 0)\n",
    "    elif frame_status == \"red\":\n",
    "        color = (0, 0, 255)\n",
    "    else:\n",
    "        color = (255, 0, 0)\n",
    "    cv2.rectangle(frame, (30, 30), (w - 30, h - 30), color, 10)\n",
    "\n",
    "    # Auth info top-left\n",
    "    cv2.putText(frame, f\"Name: {predicted_label}\", (30, 50), font, 0.8, (255, 255, 255), 2)\n",
    "    cv2.putText(frame, f\"Confidence: {confidence_percent:.2f}%\", (30, 90), font, 0.8, (255, 255, 255), 2)\n",
    "\n",
    "    # Door status bottom-right\n",
    "    if door_status:\n",
    "        (text_width, _), _ = cv2.getTextSize(door_status, font, 1, 2)\n",
    "        x_pos = w - text_width - 20\n",
    "        y_pos = h - 30\n",
    "        color = (0, 255, 0) if \"Opened\" in door_status or \"Match\" in door_status else (0, 0, 255)\n",
    "        cv2.putText(frame, door_status, (x_pos, y_pos), font, 1, color, 3)\n",
    "\n",
    "    # Show video\n",
    "    cv2.imshow(\"Face Recognition System\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        print(\"Exiting system...\")\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70347462-4089-440d-ae39-029b9c8bb434",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras_facenet import FaceNet\n",
    "from mtcnn import MTCNN\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "import time\n",
    "import pyttsx3\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "def speak(text):\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "\n",
    "# Load trained classifier and label encoder\n",
    "model = load_model(\"facenet_classifier_model.h5\")\n",
    "with open(\"label_encoder.pkl\", \"rb\") as f:\n",
    "    encoder = pickle.load(f)\n",
    "\n",
    "# Load FaceNet and MTCNN\n",
    "detector = MTCNN()\n",
    "embedder = FaceNet()\n",
    "\n",
    "# Webcam setup\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 500)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 500)\n",
    "\n",
    "print(\"Press 'q' to quit\")\n",
    "\n",
    "# Constants and state variables\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "match_interval = 5  # Check every 10 seconds\n",
    "last_match_time = 0\n",
    "match_count = 0\n",
    "max_attempts = 3\n",
    "sleeping = False\n",
    "sleep_start = 0\n",
    "sleep_timeout = 10  # Sleep after 10 sec of inactivity\n",
    "last_seen_time = time.time()\n",
    "\n",
    "match_display_time = 0\n",
    "predicted_label = \"\"\n",
    "confidence_percent = 0\n",
    "\n",
    "match_pending_door_open = Falseq\n",
    "door_open_time = 0\n",
    "show_door_closed_until = 0\n",
    "frame_status = \"blue\"\n",
    "\n",
    "# Center the OpenCV window\n",
    "cv2.namedWindow(\"Face Recognition System\", cv2.WINDOW_NORMAL)\n",
    "screen_res = (1920, 1080)\n",
    "window_width, window_height = 1000, 1000\n",
    "cv2.resizeWindow(\"Face Recognition System\", window_width, window_height)\n",
    "cv2.moveWindow(\"Face Recognition System\", (screen_res[0] - window_width) // 2, (screen_res[1] - window_height) // 2)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    current_time = time.time()\n",
    "\n",
    "    # Draw circular webcam mask\n",
    "    mask = np.zeros_like(frame)\n",
    "    h, w = frame.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "    radius = min(center) - 20\n",
    "    cv2.circle(mask, center, radius, (255, 255, 255), -1)\n",
    "    circular_frame = cv2.bitwise_and(frame, mask)\n",
    "    rgb_frame = cv2.cvtColor(circular_frame, cv2.COLOR_BGR2RGB)\n",
    "    results = detector.detect_faces(rgb_frame)\n",
    "\n",
    "    # Update last seen time if face detected\n",
    "    if results:\n",
    "        last_seen_time = current_time\n",
    "\n",
    "    # Sleep if no face for sleep_timeout\n",
    "    if not results and not sleeping and current_time - last_seen_time > sleep_timeout:\n",
    "        speak(\"Going to sleep mode\")\n",
    "        sleeping = True\n",
    "        sleep_start = current_time\n",
    "        continue\n",
    "\n",
    "    # Wake up if face appears\n",
    "    if sleeping and results:\n",
    "        speak(\"Waking up\")\n",
    "        sleeping = False\n",
    "\n",
    "    # Face matching logic (once every match_interval seconds)\n",
    "    if not sleeping and results and current_time - last_match_time > match_interval:\n",
    "        for result in results:\n",
    "            x, y, w_box, h_box = result['box']\n",
    "            x, y = max(0, x), max(0, y)\n",
    "            face = rgb_frame[y:y + h_box, x:x + w_box]\n",
    "            try:\n",
    "                face = cv2.resize(face, (160, 160))\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            embedding = embedder.embeddings([face])[0]\n",
    "            prediction = model.predict(np.expand_dims(embedding, axis=0))[0]\n",
    "            confidence = np.max(prediction)\n",
    "            predicted_label = encoder.inverse_transform([np.argmax(prediction)])[0]\n",
    "            confidence_percent = confidence * 100\n",
    "\n",
    "            last_match_time = current_time\n",
    "\n",
    "            if confidence > 0.8:\n",
    "                print(f\"[MATCH ] Name: {predicted_label}, Confidence: {confidence_percent:.2f}%\")\n",
    "                speak(f\"Match found. Hello {predicted_label}\")\n",
    "                match_display_time = current_time + 5\n",
    "                match_pending_door_open = True\n",
    "                match_time = current_time\n",
    "                frame_status = \"green\"\n",
    "                match_count = 0\n",
    "                break\n",
    "            else:\n",
    "                match_count += 1\n",
    "                speak(\"Match not found\")\n",
    "                frame_status = \"red\"\n",
    "                if match_count >= max_attempts:\n",
    "                    speak(\"Maximum attempts reached. Access denied.\")\n",
    "                    match_count = 0\n",
    "\n",
    "    # Door opening\n",
    "    if match_pending_door_open and current_time - match_time > 1:\n",
    "        speak(\"Door opened\")\n",
    "        door_open_time = current_time\n",
    "        match_pending_door_open = False\n",
    "\n",
    "    # Door closing after 15 seconds\n",
    "    if door_open_time and current_time - door_open_time > 15:\n",
    "        speak(\"Door closed\")\n",
    "        show_door_closed_until = current_time + 2\n",
    "        door_open_time = 0\n",
    "\n",
    "    # Draw frame border and info text\n",
    "    if frame_status == \"green\":\n",
    "        color = (0, 255, 0)\n",
    "    elif frame_status == \"red\":\n",
    "        color = (0, 0, 255)\n",
    "    else:\n",
    "        color = (255, 0, 0)\n",
    "\n",
    "    cv2.circle(circular_frame, center, radius, color, 10)\n",
    "\n",
    "    # Display match info (top-left corner) for 5 seconds\n",
    "    if current_time < match_display_time:\n",
    "        cv2.putText(circular_frame, f\"Name: {predicted_label}\", (20, 40), font, 0.9, (0, 255, 0), 2)\n",
    "        cv2.putText(circular_frame, f\"Confidence: {confidence_percent:.2f}%\", (20, 80), font, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "    # Display door closed message at bottom-right corner\n",
    "    if show_door_closed_until and current_time < show_door_closed_until:\n",
    "        text = \"Door Closed\"\n",
    "        (text_width, text_height), _ = cv2.getTextSize(text, font, 1, 2)\n",
    "        x_pos = w - text_width - 20\n",
    "        y_pos = h - 20\n",
    "        cv2.putText(circular_frame, text, (x_pos, y_pos), font, 1, (0, 0, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"Face Recognition System\", circular_frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ffaa15-f73f-45fc-8ddc-15f2fb893c69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04d5e3b-0532-4048-852a-5735e085295b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
